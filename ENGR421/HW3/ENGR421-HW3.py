# -*- coding: utf-8 -*-
"""ENGR421-HW3.2.ipynb adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1adzhd3--gazjE3M4fncufJgZzPYq-AGt
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""##Distribution parameters

"""

class_means = np.array([[0,2.5],[-2.5,-2.0], [+2.5,-2.0]])

class_covariances = np.array([[[3.2, 0],[0,1.2]],[[1.2,0.8],[0.8,1.2]],[[1.2,-0.8],[-0.8,1.2]]])

class_sizes = np.array([120, 80, 100])

"""##Generating and plot data"""

np.random.seed(64388)

points1 = np.random.multivariate_normal(class_means[0,:], class_covariances[0,:,:], class_sizes[0])
points2 = np.random.multivariate_normal(class_means[1,:], class_covariances[1,:,:], class_sizes[1])
points3 = np.random.multivariate_normal(class_means[2,:], class_covariances[2,:,:], class_sizes[2])

X = np.concatenate([points1, points2, points3])

#plotting data points
plt.figure(figsize = (10, 10))
plt.scatter(points1[:,0],points1[:, 1:], c= "r")
plt.scatter(points2[:,0],points2[:, 1:], c="g")
plt.scatter(points3[:,0],points3[:, 1:], c="b")
plt.xlabel("x1")
plt.ylabel("x2")

"""##Generating labels"""

#generating labels
labels = np.concatenate([np.repeat(i+1, class_sizes[i]) for i in range(len(class_sizes))])
print(labels)

no_classes = np.max(labels)
no_datapoints = np.sum(class_sizes)

# one-of-K encoding
Y_truth = np.zeros((no_datapoints, no_classes)).astype(int)
Y_truth[range(no_datapoints), labels - 1] = 1

"""##Sigmoid function"""

def sigmoid(W, w0, X):
  WtX = np.matmul(X, W)
  WtX_w0 = (WtX + w0)
  
  #subtract max values to normalize
  scores = WtX_w0 - np.repeat(np.amax(WtX_w0, axis = 1, keepdims = True), no_classes, axis = 1)

  return 1/(1 + np.exp(-scores))

"""##picking the maximum sigmoid score"""

def get_max_class(score):
  max =  np.max(score)
  class_index = -1
  
  for i in range(len(score)):
    if score[i]==max:
      class_index = i 

  class_array = np.zeros((1,no_classes))
  class_array[0,class_index] = 1

  return class_array

def generate_y_hats(scores):
  y_hats= []
  for i in range(no_datapoints):
    y_hats.append(get_max_class(scores[i]))

  return np.squeeze(np.array(y_hats).reshape(300,-1))

"""#Gradient functions"""

def gradient_W(Y, Y_hat, sig, X):
  array = []
  for i in range(no_classes):
    array.append(np.matmul((Y[:,i] -Y_hat[:,i])*sig[:,i]*(1-sig[:,i]), X))
  #print(np.matmul((Y_truth[:,i] -Y_hats[:,i])*sig[:,i]*(1-sig[:,i]), X))

  return np.asarray(array).transpose()


def gradient_w0(Y, Y_hat, sig):
  array = []
  for i in range(no_classes):
    array.append(np.sum((Y[:,i] -Y_hat[:,i])*sig[:,i]*(1-sig[:,i]), axis = 0))

  return np.asarray(array).transpose()

"""#Gradient Descent"""

W = np.random.uniform(low = -0.01, high = 0.01, size = (X.shape[1], no_classes))
w0 = np.random.uniform(low = -0.01, high = 0.01, size = (1, no_classes))

eta = 0.01
epsilon = 0.001

iteration = 0
objective_values = []


for i in range(100):
    sig = sigmoid(W, w0, X)
    Y_hats = generate_y_hats(sig)

    objective_values = np.append(objective_values, 0.5*np.sum((Y_truth - Y_hats)**2))

    W = W + eta * gradient_W(Y_truth, Y_hats, sig, X)
    w0 = w0 + eta * gradient_w0(Y_truth, Y_hats, sig)

    iteration = iteration + 1
  
print(iteration)
print(W)
print(w0)

"""#Convergence of the error"""

plt.figure(figsize = (10, 6))
plt.plot(range(1, iteration + 1), objective_values, "k-")
plt.xlabel("Iteration")
plt.ylabel("Error")
plt.show()

"""#Constructing the confusion matrix"""

def one_k_to_normal(Y_hats):
  result = []
  for i in range(no_datapoints):
    for c in range(no_classes):
      if Y_hats[i,c] == 1:
        result.append(c+1)
  
  return np.asarray(result)

y_hats_labels = one_k_to_normal(Y_hats)

confusion_matrix = pd.crosstab(y_hats_labels, labels, rownames = ['y_pred'], colnames = ['y_truth'])
print(confusion_matrix)

"""##Spotting the wrong classifications on plot"""

plt.figure(figsize = (10, 10))
plt.scatter(points1[:,0],points1[:, 1], c= "r")
plt.scatter(points2[:,0],points2[:, 1], c="g")
plt.scatter(points3[:,0],points3[:, 1], c="b")

x1_interval = np.linspace(-7, +7, 1001)
x2_interval = np.linspace(-6, +6, 1001)
x1_grid, x2_grid = np.meshgrid(x1_interval, x2_interval)

discriminant_values = np.zeros((len(x1_interval), len(x2_interval), no_classes))

for c in range(no_classes):
    discriminant_values[:,:,c] = W[0, c] * x1_grid + W[1, c] * x2_grid + w0[0, c]

class1 = discriminant_values[:,:,0]
class2 = discriminant_values[:,:,1]
class3 = discriminant_values[:,:,2]

class1[(class1< class2) & (class1 < class3)] = np.nan
class2[(class2 < class1) & (class2 < class3)] = np.nan
class3[(class3 < class1) & (class3 < class2)] = np.nan

discriminant_values[:,:,0] = class1
discriminant_values[:,:,1] = class2
discriminant_values[:,:,2] = class3

plt.contour(x1_grid, x2_grid, discriminant_values[:,:,0] - discriminant_values[:,:,1], levels = 0, colors = "k")
plt.contour(x1_grid, x2_grid, discriminant_values[:,:,0] - discriminant_values[:,:,2], levels = 0, colors = "k")
plt.contour(x1_grid, x2_grid, discriminant_values[:,:,1] - discriminant_values[:,:,2], levels = 0, colors = "k")

plt.plot(X[y_hats_labels != labels, 0], X[y_hats_labels != labels, 1], "ko", markersize = 14, fillstyle = "none")

plt.show()