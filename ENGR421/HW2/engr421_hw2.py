# -*- coding: utf-8 -*-
"""ENGR421-HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JVFy8T9AFFe3yN6Bp_G2Toi6JRD-APw2
"""

import pandas as pd
import numpy as np
import random
import math

"""#Loading files"""

def safelog(x):
  return np.log(x + 1e-100)

images = pd.read_csv("/content/drive/MyDrive/hw02_images.csv", header =None)
labels = pd.read_csv("/content/drive/MyDrive/hw02_labels.csv", dtype = int, header =None)
labels = np.squeeze(np.array(labels))

no_classes = np.max(labels)
data_size = images.shape[1]
#####
print(no_classes)
print(labels)

#####

"""#Train/test split"""

train_X = np.array(images[0:30000])
test_X = np.array(images[30000:])

train_Y = np.array(labels[0:30000])
test_Y = np.array(labels[30000:])

class_size_train = np.bincount(train_Y)
class_size_test = np.bincount(test_Y)


#####

print(class_size_test)
#####

"""#Estimate means"""

def calculate_means(X, Y, class_sizes):

  sample_means = []
  for i in range(no_classes):
     sample_means.append(np.sum(X[Y == i+1], axis = 0, keepdims = True)/class_sizes[i+1])

  return np.squeeze(sample_means)

means_train = calculate_means(train_X, train_Y, class_size_train)
means_test = calculate_means(test_X, test_Y, class_size_test)


#####
print(means_train.shape)

#####

"""#Estimate deviations"""

def calculate_deviations(X, Y,sample_means,class_sizes):
  
  sample_deviations = []
  for i in range(1,no_classes+1):
    points = X[Y == i] 
    sample_deviations.append(np.sqrt(np.sum((points - sample_means[i-1])**2, axis = 0)/class_sizes[i]))

  return np.reshape(sample_deviations, (no_classes, -1))

deviations_train = calculate_deviations(train_X, train_Y, means_train, class_size_train)
deviations_test = calculate_deviations(test_X, test_Y, means_test, class_size_test)

print(deviations_test.shape)
#since some values on the test deviation came out ot be 0, I set those values to a very small positive value
deviation_test_safe = np.stack([[1e-150 if deviations_test[k][i]==0 else deviations_test[k][i] for i in range(data_size)] for k in range(no_classes)])

"""#Estimate prior probabilities"""

prior_train = [len(train_X[train_Y == i +1])/ len(train_X) for i in range(no_classes)]
prior_test= [len(test_X[test_Y == i +1])/ len(test_X) for i in range(no_classes)]

print(prior_train)
print(prior_test)

"""#Naive Bayes Classification"""

def naive_bayes_score(X,deviation, mean, prior):
  image_scores = np.sum([(- 0.5 * safelog(2 * math.pi * deviation[i]**2) - 0.5 * (X - mean[i])**2 / deviation[i]**2) + safelog(prior[i]) for i in range(no_classes)], axis=2)
  return image_scores

def get_max_class(array):
  max = np.max(array)
  index = 0
  for i in range(len(array)):
    if max == array[i]:
      index = i+1
  
  return index

def generate_predictions(scores):
  y_hats = []
  no_data = len(scores)
  
  for i in range(no_data):
    y_hats.append(get_max_class(scores[i]))
    
  return np.array(y_hats)

"""#Estimations for Training set"""

scores_per_pixel_train = naive_bayes_score(train_X, deviations_train, means_train, prior_train)

print(scores_per_pixel_train.shape)
image_scores_train = np.transpose(scores_per_pixel_train)
print(image_scores_train.shape)

y_pred_train = generate_predictions(image_scores_train)


confusion_matrix = pd.crosstab(y_pred_train, train_Y, rownames = ['y_pred'], colnames = ['y_truth'])
print(confusion_matrix)

"""#Estimations for Test Set"""

scores_per_pixel_test = naive_bayes_score(test_X, deviation_test_safe, means_test, prior_test) 


image_scores_test = np.transpose(scores_per_pixel_test)

y_pred_test = generate_predictions(image_scores_test)


confusion_matrix = pd.crosstab(y_pred_test, test_Y, rownames = ['y_pred'], colnames = ['y_truth'])
print(confusion_matrix)