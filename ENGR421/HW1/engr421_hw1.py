# -*- coding: utf-8 -*-
"""ENGR421-HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ZyelEPDGs74ZiXqkaWiArAM54BEFQxj
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import math
import pandas as pd

class_means = np.array([[0.0,2.5], [-2.5,-2.0], [2.5,-2.0]])

class_covariances = [np.vstack([[3.2, 0.0],[0.0,1.2]]), np.vstack([[1.2, 0.8], [0.8, 1.2]]), np.vstack([[1.2, -0.8], [-0.8,1.2]])]

class_sizes = np.array([120,80,100])

np.random.seed(1)
points1 = np.random.multivariate_normal(class_means[0], class_covariances[0], size = class_sizes[0])
points2 = np.random.multivariate_normal(class_means[1], class_covariances[1], size = class_sizes[1])
points3 = np.random.multivariate_normal(class_means[2], class_covariances[2], size = class_sizes[2])


plt.figure(figsize = (8, 8))
plt.scatter(points1[:,0],points1[:, 1:], c= "r")
plt.scatter(points2[:,0],points2[:, 1:], c="g")
plt.scatter(points3[:,0],points3[:, 1:], c="b")
plt.xlabel("x1")
plt.ylabel("x2")

"""#calculating sample means"""

sample_means = [sum(points1) / class_sizes[0], sum(points2) / class_sizes[1], sum(points3) / class_sizes[2]]

"""#calculating sample covariance matrices"""

def calculate_covariances(points,sample_mean, class_size):
  sum = 0
  for i in range(class_size):
    sum+=np.dot(np.transpose([points1[i]-sample_means[0]]), ([points1[i]-sample_means[0]]))
  return sum/class_size

sample_covariances = [calculate_covariances(points1,sample_means[0],class_sizes[0]), calculate_covariances(points2,sample_means[1],class_sizes[1]), 
                      calculate_covariances(points3,sample_means[2],class_sizes[2])]

print(sample_covariances[0])

"""#calculating prior probabilities"""

N = sum(class_sizes)
prior_probabilities = [class_sizes[0]/N, class_sizes[1]/N, class_sizes[2]/N]
print(prior_probabilities)

"""#calculating the scoring functions"""

def get_score(points, sample_mean, covariance, prior):
  D = np.shape(points)[1]
  
  score = np.vstack([-D/2 * np.log(2*math.pi) - 1/2 * np.log(np.linalg.det(covariance)) - 1/2*(np.dot(np.dot((points[i] - sample_mean),np.linalg.inv(covariance)),np.transpose(points[i]-sample_mean)))
  + np.log(prior) for i in range(len(points))])
  return score

#calculate scores 
points=np.concatenate([points1,points2, points3])

labels = np.hstack([np.repeat(1,class_sizes[0]), np.repeat(2,class_sizes[1]),np.repeat(3,class_sizes[2])])

g1 = get_score(points,sample_means[0],sample_covariances[0], prior_probabilities[0])
g2 = get_score(points,sample_means[1],sample_covariances[1], prior_probabilities[1])
g3 = get_score(points,sample_means[2],sample_covariances[2], prior_probabilities[2])

def assign_y_pred(g1,g2,g3):
  y_pred = []
  for i in range(len(labels)):
    if g1[i] > g2[i] and g1[i]>g3[i]:
      y_pred.append(1)
  
    if g2[i] > g1[i] and g2[i]>g3[i]:
      y_pred.append(2)

    if g3[i] > g2[i] and g3[i]>g1[i]:
      y_pred.append(3)
  return y_pred

y_pred = np.array(assign_y_pred(g1,g2,g3))
print(np.shape(y_pred))
print(np.shape(labels))

confusion_matrix = pd.crosstab(y_pred, labels, rownames = ['y_pred'], colnames = ['y_truth'])
print(confusion_matrix)